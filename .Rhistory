return(matrix(sapply(trajectList,function(x) SimilarityMeasures::Frechet(x[[1]],x[[2]],testLeash=testLeash)),ncol=n,nrow=n))
} else {
cl=parallel::makeCluster(cl)
return(matrix(parallel::parSapply(cl,trajectList,function(x) SimilarityMeasures::Frechet(x[[1]],x[[2]])),ncol=n,nrow=n))
ntdist=matrix(parallel::parSapply(cl,trajectList,function(x) SimilarityMeasures::Frechet(x[[1]],x[[2]])),ncol=n,nrow=n)
parallel::stopCluster(cl)}
}
mFrechet=function(data,parallel=FALSE,cl=NULL,testLeash=-1)
{
checkmate::assertList(data)
checkmate::assertLogical(parallel)
if(!is.null(cl)){
if(!isTRUE(parallel)){
stop("Error: parallel should be TRUE")}
checkmate::assertNumeric(cl,lower=1)}
d=length(data)
if(sum(apply(sapply(data,dim),1,diff))!=0){
stop("Error: objects in data have different dimensions")}
if(sum(sapply(data,function(x)sum(is.na(x))))!=0){
warning("data have missing values; some distances cannot be computed.")}
n=unique(sapply(data,ncol))
t=unique(sapply(data,nrow))
d= length(data)
# 3d matrix of data
data_array= array(unlist(data), dim=c(t,n,d))
#list of indices for upper rtriangular distances matrix
combs_lower_tri = combn(1:n, 2, simplify=FALSE)
if(parallel==FALSE){
#compute results of upper/lower triangle of distance matrix
lower_tri_res = sapply(combs_lower_tri,
function(x) SimilarityMeasures::Frechet(data_array[,x[1],],
data_array[,x[2],] ,
testLeash=testLeash))
} else {
cl=parallel::makeCluster(cl)
parallel::clusterExport(cl,list("data", "testLeash"))
#compute results of upper/lower triangle of distance matrix
lower_tri_res = parallel::parSapply(cl,
combs_lower_tri,
function(x) SimilarityMeasures::Frechet(data_array[,x[1],],
data_array[,x[2],] ,
testLeash=testLeash))
parallel::stopCluster(cl)
}
#arrange results into distance matrix
result_matrix = matrix(NA, nrow = n, ncol = n)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
result_matrix = t(result_matrix)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
diag(result_matrix)= rep(0, n)
return(result_matrix)
}
Old = mFrechetOld(dist)
new = mFrechet(dist)
Old = mFrechetOld(dist)
new = mFrechet(dist)
dist = list()
for (i in (1:3)){
dist[[i]] = matrix(rnorm(16), nrow = 4, ncol = 4)
}
dist
mFrechetOld(dist)
mFrechet(dist)
Old = mFrechetOld(dist)
Old
new = mFrechet(dist)
new
install.packages("Rfast")
Occurences <- list()
X1 <- c(1,2,1,1)
X2 <- c(1,3,1,1)
X3 <- c(1,4,1,1)
Y1 <- c(1,2,1,1)
Y2 <- c(1,3,1,1)
Y3 <- c(1,4,1,1)
Z1 <- c(1,2,1,1)
Z2 <- c(1,3,1,1)
Z3 <- c(1,4,1,1)
Occurences[[1]] <- cbind(X1, X2, X3)
Occurences[[2]] <- cbind(Y1, Y2, Y3)
Occurences[[3]] <- cbind(Z1, Z2, Z3)
Occurences
dim(1)
?dim
data = matrix(1:16, ncol = 4, nrow = 4)
Maxs= Rfast::colMaxs(data)
Maxs
data
Rfast::colMaxs(data)
?RFasr::colMaxs
?RFast::colMaxs
?Rfast::colMaxs
Rfast::colMaxs(data, value =TRUE)
?combn
n=4
combs_lower_tri = utils::combn(1:n, 2, simplify=FALSE)
lower_tri_res = sapply(combs_lower_tri,
function(x) abs(Maxs[x[[1]]]-Maxs[x[[2]]]) )
lower_tri_res
Maxs
Maxs= Rfast::colMaxs(data, value =TRUE)
Maxs
combs_lower_tri = utils::combn(1:n, 2, simplify=FALSE) #combinations for indexes of upper triangular matrix
lower_tri_res = sapply(combs_lower_tri,
function(x) abs(Maxs[x[[1]]]-Maxs[x[[2]]]) )
lower_tri_res
mglobMax3=function(data,parallel=FALSE,cl=NULL)
{
checkmate::assertMatrix(data)
checkmate::assertLogical(parallel)
if(!is.null(cl)){
if(!isTRUE(parallel)){
stop("Error: parallel should be TRUE")}
checkmate::assertNumeric(cl,lower=1)}
if(sum(sapply(data,function(x)sum(is.na(x))))!=0){
warning("data have missing values; some distances cannot be computed.")}
n=unique(sapply(data,ncol))
if(parallel==FALSE){
Maxs= Rfast::colMaxs(data, value =TRUE) # returns maximum per column
combs_lower_tri = utils::combn(1:n, 2, simplify=FALSE) #combinations for indexes of upper triangular matrix
lower_tri_res = sapply(combs_lower_tri,function(x) abs(Maxs[x[[1]]]-Maxs[x[[2]]]) ) # compute results for upper triangular distance matrix
} else {
cl=parallel::makeCluster(cl)
parallel::clusterExport(cl,list("Maxs"))
#compute results of upper/lower triangle of distance matrix
lower_tri_res = parallel::parSapply(cl,
combs_lower_tri,
function(x) abs(Maxs[x[[1]]]-Maxs[x[[2]]]) ) # compute results for upper triangular distance matrix
parallel::stopCluster(cl)
}
#format results
result_matrix = matrix(NA, nrow = n, ncol = n)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
result_matrix = t(result_matrix)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
diag(result_matrix)= rep(0, n)
return(result_matrix)
}
mglobMax3(data)
data
ncol(data)
mglobMax3=function(data,parallel=FALSE,cl=NULL)
{
checkmate::assertMatrix(data)
checkmate::assertLogical(parallel)
if(!is.null(cl)){
if(!isTRUE(parallel)){
stop("Error: parallel should be TRUE")}
checkmate::assertNumeric(cl,lower=1)}
if(sum(sapply(data,function(x)sum(is.na(x))))!=0){
warning("data have missing values; some distances cannot be computed.")}
n=ncol(data)
if(parallel==FALSE){
Maxs= Rfast::colMaxs(data, value =TRUE) # returns maximum per column
combs_lower_tri = utils::combn(1:n, 2, simplify=FALSE) #combinations for indexes of upper triangular matrix
lower_tri_res = sapply(combs_lower_tri,function(x) abs(Maxs[x[[1]]]-Maxs[x[[2]]]) ) # compute results for upper triangular distance matrix
} else {
cl=parallel::makeCluster(cl)
parallel::clusterExport(cl,list("Maxs"))
#compute results of upper/lower triangle of distance matrix
lower_tri_res = parallel::parSapply(cl,
combs_lower_tri,
function(x) abs(Maxs[x[[1]]]-Maxs[x[[2]]]) ) # compute results for upper triangular distance matrix
parallel::stopCluster(cl)
}
#format results
result_matrix = matrix(NA, nrow = n, ncol = n)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
result_matrix = t(result_matrix)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
diag(result_matrix)= rep(0, n)
return(result_matrix)
}
mglobMax3(data)
data
mglobMax3(data, parallel = TRUE, cl = 3)
3!=2
positionlist[[1]] = matrix(1:16, nrow = 4, ncols= 4)
positionlist[[2]] = matrix(101:116, nrow = 4, ncols= 4)
positionlist[[1]] = matrix(1:16, nrow = 4, ncol= 4)
positionlist[[2]] = matrix(101:116, nrow = 4, ncol= 4)
positionlist=list()
positionlist[[1]] = matrix(1:16, nrow = 4, ncol= 4)
positionlist[[2]] = matrix(101:116, nrow = 4, ncol= 4)
mglobMax3(positionlist)
mglobMax2(positionlist)
mglobMax2=function(data,parallel=FALSE,cl=NULL)
{
checkmate::assertList(data)
checkmate::assertLogical(parallel)
if(!is.null(cl)){
if(!isTRUE(parallel)){
stop("Error: parallel should be TRUE")}
checkmate::assertNumeric(cl,lower=1)}
d=length(data)
if(d != 2){stop("Error: objects in data do not have 2 dimensions")}
if(sum(apply(sapply(data,dim),1,diff))!=0){
stop("Error: objects in data have different dimensions")}
if(sum(sapply(data,function(x)sum(is.na(x))))!=0){
warning("data have missing values; some distances cannot be computed.")}
n=unique(sapply(data,ncol))
if(parallel==FALSE){
return(abs(mglobMax3(data[[1]])-mglobMax3(data[[2]])))
} else {
return(abs(mglobMax3(data[[1]], parallel = parallel, cl = cl)-mglobMax3(data[[2]], parallel = parallel, cl = cl )))
}
}
mglobMax2(positionlist)
positionlist[[1]]
positionlist[[2]]
mglobMax2(positionlist, parallel = TRUE, cl = 3)
mglobMax3(positionlist[[2]], parallel = TRUE, cl = 3)
mglobMax3(positionlist[[1]], parallel = TRUE, cl = 3)
positionlist[[3]] = matrix(101:116, nrow = 4, ncol= 4)
mglobMax3(positionlist, parallel = TRUE, cl = 3)
mglobMax2(positionlist, parallel = TRUE, cl = 3)
#' Global Minimum distance between multivariate functions
#'
#' Computes the Global Minimum distance for all pairs of \eqn{m}-dimensional functions. For a single pair of functions,
#' the present R function returns the minimum euclidean distance between the function values at equal time points.
#'
#' @param data a matrix that stores a dimension of the set of functions, such that columns are individuals (\eqn{n}) and rows are discrete-time points (\eqn{t}). Functions' values should be of the same time points.
#' @param parallel logical value indicating whether computations should be parallelized. Default is FALSE. If TRUE, parallelization is conducted with \href{https://www.rdocumentation.org/packages/parallel}{parallel} package.
#' @param cl a cluster object created by \href{https://www.rdocumentation.org/packages/parallel}{parallel}. Default is NULL.
#' @return Returns a square and symmetric \eqn{n x n} matrix of \eqn{m}-dimensional global minimum distances.
#' @details For each pair of functions f and g, the present R function computes: \eqn{min t [Euclidean_Distance(f(t), g(t))]}
#' @seealso See \code{\link[parallel:makeCluster]{makeCluster}}, \code{\link[parallel:clusterExport]{clusterExport}}, \code{\link[parallel:stopCluster]{stopCluster}}, \code{\link[parallel:parApply]{parApply}} and \code{\link[parallel:parLapply]{parLapply}} from  \href{https://www.rdocumentation.org/packages/parallel}{parallel}, and \code{\link[proxy:dist]{dist}} from \href{https://cran.r-project.org/web/packages/proxy/index.html}{proxy}
#' @examples
#' ## 2-dimensional functions
#'
#' x = replicate(4, rnorm(100, 0, 3))
#' y = replicate(4, rnorm(100, 3, 1))
#' data = list(x, y)
#' mglobmin(data, parallel = FALSE, cl = NULL)
#'
#' ## 3-dimensional functions
#'
#' z = replicate(4, rpois(100, 2))
#' data = list(x, y, z)
#' mglobmin(data, parallel = FALSE, cl = NULL)
#'
#' @export
mglobMin3=function(data,parallel=FALSE,cl=NULL)
{
checkmate::assertMatrix(data)
checkmate::assertLogical(parallel)
if(!is.null(cl)){
if(!isTRUE(parallel)){
stop("Error: parallel should be TRUE")}
checkmate::assertNumeric(cl,lower=1)}
if(sum(sapply(data,function(x)sum(is.na(x))))!=0){
warning("data have missing values; some distances cannot be computed.")}
n=ncol(data)
if(parallel==FALSE){
Mins= Rfast::colMins(data, value =TRUE) # returns minimum per column
combs_lower_tri = utils::combn(1:n, 2, simplify=FALSE) #combinations for indexes of upper triangular matrix
lower_tri_res = sapply(combs_lower_tri,function(x) abs(Mins[x[[1]]]-Mins[x[[2]]]) ) # compute results for upper triangular distance matrix
} else {
cl=parallel::makeCluster(cl)
parallel::clusterExport(cl,list("Mins"))
#compute results of upper/lower triangle of distance matrix
lower_tri_res = parallel::parSapply(cl,
combs_lower_tri,
function(x) abs(Mins[x[[1]]]-Mins[x[[2]]]) ) # compute results for upper triangular distance matrix
parallel::stopCluster(cl)
}
#format results
result_matrix = matrix(NA, nrow = n, ncol = n)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
result_matrix = t(result_matrix)
result_matrix[lower.tri(result_matrix)] = lower_tri_res
diag(result_matrix)= rep(0, n)
return(result_matrix)
}
#' @export
mglobMin2=function(data,parallel=FALSE,cl=NULL)
{
checkmate::assertList(data)
checkmate::assertLogical(parallel)
if(!is.null(cl)){
if(!isTRUE(parallel)){
stop("Error: parallel should be TRUE")}
checkmate::assertNumeric(cl,lower=1)}
d=length(data)
if(d != 2){stop("Error: objects in data do not have 2 dimensions")}
if(sum(apply(sapply(data,dim),1,diff))!=0){
stop("Error: objects in data have different dimensions")}
if(sum(sapply(data,function(x)sum(is.na(x))))!=0){
warning("data have missing values; some distances cannot be computed.")}
n=unique(sapply(data,ncol))
if(parallel==FALSE){
return(abs(mglobMin3(data[[1]])-mglobMin3(data[[2]])))
} else {
return(abs(mglobMin3(data[[1]], parallel = parallel, cl = cl)-mglobMin3(data[[2]], parallel = parallel, cl = cl )))
}
}
mglobMin3(data)
data
positionlist2[[1]]=positionlist[[1]]
positionlist2=list()
positionlist2[[1]]=positionlist[[1]]
positionlist2[[2]]=positionlist[[2]]
mglobMin2(positionlist2)
load("U:/classiMultiFunc/Application/CV Results/Results_new_Frechet_deriv1")
load("U:/classiMultiFunc/Application/CV Results/Results_new_Frechet_deriv1.Rdata")
View(result_list)
result_list[[1]]
f <- function(x){ 1/x^2}
curve(f)
require(distr)
install.packages(distr)
require(distr)
install.packages("distr")
require(distr)
F = AbscontDistribution(d = function(x){ 1/x^2})
F = AbscontDistribution(d = function(x){ 1/x^2}, from = -Inf, to = +Inf)
?AbscontDistribution
setwd("C:/Users/felix/Documents/GitHub/multivariate-methods")
load("./Code/Data_prep/preproc_data.Rdata")
library("mlr")
library("LiblineaR")
library("magrittr")
library("dplyr")
#library("caret")
#data preprocess specific to logit
#add squared variables
df %<>%
mutate(
disbursed_amount2 = disbursed_amount^2,
asset_cost2 = asset_cost^2,
ltv2 = ltv^2,
AVERAGE.ACCT.AGE2 = AVERAGE.ACCT.AGE^2,
CREDIT.HISTORY.LENGTH2 = CREDIT.HISTORY.LENGTH^2
)
#change factor to dummy
df = createDummyFeatures(
df,
target = "loan_default",
method = "1-of-n",
cols = NULL
)
#specify cross validation with necessary steps:
classif.task = makeClassifTask(id = "CreditScoring",
data = df,
target = "loan_default")
classif.lrn = makeLearner("classif.LiblineaRL1LogReg",
predict.type = "prob",
fix.factors.prediction = TRUE)
#standardize wrapper
classif.lrn = makePreprocWrapperCaret(classif.lrn, method = list( disbursed_amount = "norm"))
#imputation wrapper
classif.lrn = makeImputeWrapper(classif.lrn,
list(numeric = imputeMedian(),
integer = imputeMedian(),
factor  = imputeMode()),
dummy.type = "numeric"
)
#rdesc = makeResampleDesc("CV", iters = 2, stratify=TRUE)
#resample(classif.lrn, classif.task , rdesc, measures = list(auc),models = TRUE)
mod = train(classif.lrn, task = classif.task )
pred = predict(mod, task = classif.task)
cal = generateCalibrationData(pred,breaks = seq(from= 0, to =1, by = 0.1))
plotCalibration(cal, rag=FALSE)
#undersampling--> Not useful actually
task.under = undersample(classif.task, rate = 0.2)
mod = mlr::train(classif.lrn, task =task.under )
pred = predict(mod, task = task.under)
cal = generateCalibrationData(pred,breaks = seq(from= 0, to =1, by = 0.1))
plotCalibration(cal, rag=FALSE)
setwd("C:/Users/felix/Documents/GitHub/multivariate-methods")
load("./Code/Data_prep/preproc_data.Rdata")
library("mlr")
library("LiblineaR")
library("magrittr")
library("dplyr")
#library("caret")
#data preprocess specific to logit
#add squared variables
df %<>%
mutate(
disbursed_amount2 = disbursed_amount^2,
asset_cost2 = asset_cost^2,
ltv2 = ltv^2,
AVERAGE.ACCT.AGE2 = AVERAGE.ACCT.AGE^2,
CREDIT.HISTORY.LENGTH2 = CREDIT.HISTORY.LENGTH^2
)
#change factor to dummy
df = createDummyFeatures(
df,
target = "loan_default",
method = "1-of-n",
cols = NULL
)
#specify cross validation with necessary steps:
classif.task = makeClassifTask(id = "CreditScoring",
data = df,
target = "loan_default")
classif.lrn = makeLearner("classif.LiblineaRL1LogReg",
predict.type = "prob",
fix.factors.prediction = TRUE)
#standardize wrapper
classif.lrn = makePreprocWrapperCaret(classif.lrn, method = list( disbursed_amount = "norm"))
#imputation wrapper
classif.lrn = makeImputeWrapper(classif.lrn,
list(numeric = imputeMedian(),
integer = imputeMedian(),
factor  = imputeMode()),
dummy.type = "numeric"
)
rdesc = makeResampleDesc("CV", iters = 2, stratify=TRUE)
resample(classif.lrn, classif.task , rdesc, measures = list(auc),models = TRUE)
View(mod)
View(mod)
rm(list=ls())
setwd("C:/Users/felix/Documents/GitHub/multivariate-methods")
load("./Code/Data_prep/preproc_data.Rdata")
library("mlr")
library("LiblineaR")
library("magrittr")
library("dplyr")
#library("caret")
#data preprocess specific to logit
#add squared variables
df %<>%
mutate(
disbursed_amount2 = disbursed_amount^2,
asset_cost2 = asset_cost^2,
ltv2 = ltv^2,
AVERAGE.ACCT.AGE2 = AVERAGE.ACCT.AGE^2,
CREDIT.HISTORY.LENGTH2 = CREDIT.HISTORY.LENGTH^2
)
#change factor to dummy
df = createDummyFeatures(
df,
target = "loan_default",
method = "1-of-n",
cols = NULL
)
#specify cross validation with necessary steps:
classif.task = makeClassifTask(id = "CreditScoring",
data = df,
target = "loan_default")
classif.lrn = makeLearner("classif.LiblineaRL1LogReg",
predict.type = "prob",
fix.factors.prediction = TRUE)
#standardize wrapper
classif.lrn = makePreprocWrapperCaret(classif.lrn, method = list( disbursed_amount = "norm"))
#imputation wrapper
classif.lrn = makeImputeWrapper(classif.lrn,
list(numeric = imputeMedian(),
integer = imputeMedian(),
factor  = imputeMode()),
dummy.type = "numeric"
)
rdesc = makeResampleDesc("CV", iters = 2, stratify=TRUE)
resample(classif.lrn, classif.task , rdesc, measures = list(auc),models = TRUE)
mod = train(classif.lrn, task = classif.task )
pred = predict(mod, task = classif.task)
cal = generateCalibrationData(pred,breaks = seq(from= 0, to =1, by = 0.1))
plotCalibration(cal, rag=FALSE)
cal = generateCalibrationData(pred,breaks = seq(from= 0, to =1, by = 0.05))
plotCalibration(cal, rag=FALSE)
cal
?generateCalibrationData
?makeLearner
str(df)
pred
?pred
?predict
?mlr::predictLearner
pred2 = predict(mod, task = classif.task)[, -c("prob.0")]
pred
pred2 = predict(mod, task = classif.task)$data[, -c("prob.0")]
pred$data
pred$data[,-"prob.0"]
pred$data[,c("prob.0")]
pred2 = pred
pred2$data = predict(mod, task = classif.task)$data[, -c("prob.0")]
pred$data[,c("prob.0")]
pred$data
pred = predict(mod, task = classif.task)
pred2 = pred
pred2$data = predict(mod, task = classif.task)$data[, c("id","truth", "prob.1", "response")]
pred2$data
cal = generateCalibrationData(pred,breaks = seq(from= 0, to =1, by = 0.05))
plotCalibration(cal, rag=FALSE)
cal = generateCalibrationData(pred2,breaks = seq(from= 0, to =1, by = 0.05))
plotCalibration(cal, rag=FALSE)
cal
hist(pred$data$prob.1)
cal = generateCalibrationData(pred2,breaks = c(0, seq(from= 0.4, to =1, by = 0.05)))
plotCalibration(cal, rag=FALSE)
cal = generateCalibrationData(pred,breaks = c(0, seq(from= 0.4, to =1, by = 0.05)))
plotCalibration(cal, rag=FALSE)
hist(pred$data$prob.1)
sum(pred$data$prob.1>0.5)
sum(pred$data$prob.1>0.4)
sum(pred$data$prob.1>0.6)
sum(pred$data$prob.1>0.7)
sum(pred$data$prob.1>0.8)
sum(pred$data$prob.1>0.65)
sum(pred$data$prob.1>0.6)
plotCalibration(cal, rag=FALSE)
cal = generateCalibrationData(pred,breaks = c(0, seq(from= 0.5, to =1, by = 0.05)))
plotCalibration(cal, rag=FALSE)
df[(pred$data$prob.1>0.6),]
df[(pred$data$prob.1>0.8),]
View(mod)
View(mod)
calculateROCMeasures(pred)
Roc_data = generateThreshVsPerfData(pred, measures = list(fpr, tpr, mmce))
plotROCCurves(Roc_data)
